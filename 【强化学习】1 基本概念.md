---
author: [Even]
date: [2026年02月02日]
update: [2026年02月02日]
title: [【强化学习】1 基本概念]
tags: [强化学习,数学原理,概念]
---

# 1 基本概念

## 1.1 grid-world example
- 网格世界的机器人
- 找到到达目标的最优策略
  - 尽快到达
  - 不重复拐弯
  - 不超过边界
  - ...
## 1.2 State
1. Agent相对于环境的状态

- State Space: 状态的集合（状态空间）

## 1.3 Action
1. agent可采取的动作，是状态的函数$A(S_i)$

- Action space of a state: 一个状态下所有可采取的action的集合

## 1.4 State transition
1. 采取Action后，agent发生从一个状态切换为另一个状态的过程称为状态转换

- Forbidden area
  - 可进入，但会受惩罚（一般情况）
  - 不可进入
- Tabular representation: 使用表格表述state transition
- state transition probability: 条件概率用于描述state transition，更一般化
  - deterministic 可以是确定性的
  - stochastic 也可是随机性的

## 1.5 Policy
1. 策略，对二维网格来说可以是一系列的移动箭头信号
2. 数学表述可使用条件概率表示，编程时常用tabular representation，所有概率加起来应该为1
3. 策略可以是确定性的，也可以是随机性的
4. 策略决定行动：给定状态s，策略$\pi$给出动作的概率分布，agent按此分布采样得到具体行动a
5. 行动验证策略：通过不断执行行动并观察奖励与下一状态，agent可以评估并优化策略，使其长期累积奖励最大化
6. 共同构成“序贯决策”：策略与行动循环，形成“试错-反馈-改进”闭环

- paths/trajectory

## 1.6 Reward
1. 奖励
2. 没有punishment相当于鼓励
3. reward可以结合到HMI，方便查看
4. tabular representation不太适用于reward表示
5. 仍是条件概率适合表示$p(r=-1|s_1,a_1)$ and $p(r\neq -1|s_1)=0$

## 1.7 Trajectory and return
trajectory是state-action-reward chain

$$s_1\xrightarrow[r=0]{a_3}s_4\xrightarrow[r=1]{a_3}s_7\xrightarrow[r=0]{a_2}s_8\xrightarrow[r=\pm 1]{a_2}s_9$$

## 1.8 Discounted return
1. 引入折扣率discounted rate $\gamma \in [0,1)$ 防止不收敛
2. $\text{discounted count} = 0 + \gamma 0 + \gamma^2 1 + \gamma^4 1 + \gamma^5 1 + ...= \gamma^3(1+\gamma+\gamma^2+...)=\gamma^3\dfrac{1}{1-\gamma}$
3. $\gamma \to 0$，更注重短期奖励
4. $\gamma \to 1$，更注重长期奖励

## 1.9 Episode
1. 有些任务具有terminal states
2. continuing tasks没有tetminal states
3. 法1：target state是一种特殊的absorbing state，后续rewards均为0，从而将episodic task转换为continuing task
4. 法2：将target state作为一种正常的state，agent进入该state后仍会因获取更高的reward离开，但通常policy保证了agent会停在里面（一般化）

## 1.10 Markov decision process(MDP)
1. 集合
   - State: $\mathcal{S}$
   - Action: $\mathcal A(s), s\in\mathcal{S}$
   - Reward: $\mathcal{R}(s,a)$
2. 概率分布
    - 状态转换概率：在state $s$，采取action $a$， 则转换为状态$s'$的概率为$p(s'|s,a)$
    - 奖励概率：在state $s$，采取action $a$，则获得奖励$r$的概率为$p(r|s,a)$
3. 策略
    - 在状态$s$，采取行动$a$的概率为$\pi(a|s)$
4. 马尔可夫性质：无记忆性

$$p(s_{t+1}|a_{t+1},s_t,\ldots,a_1,s_0)=p(s_{t+1}|a_{t+1},s_t),$$

$$p(r_{t+1}|a_{t+1},s_{t},\ldots,a_{1},s_{0})=p(r_{t+1}|a_{t+1},s_{t}).$$