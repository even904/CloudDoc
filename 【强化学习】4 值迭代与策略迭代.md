---
author: [Even]
date: [2026年02月28日]
update: [2026年02月28日]
title: [【强化学习】4 值迭代与策略迭代]
tags: [强化学习,数学原理,值迭代,策略迭代,截断策略迭代]
---

# 4 值迭代与策略迭代

## 4.1 Value iteration algorithm

- 解决BOE问题
    - BOE表达式
    $$v=f(v)=\max_\pi(r_\pi+\gamma P_\pi v)$$
    - Contraction mapping theorem
    $$v_{k+1}=f(v_k)=\max_\pi(r_\pi+\gamma P_\pi v_k), \quad k=1,2,3...$$
    其中$v_0$可为任意值
    - 以上算法称为值迭代，最终可以找到最优值和最优策略
    - 以上算法可以分为两步完成
    - Step1: policy update
    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k)$$
    其中$v_k$给定
    - Step2: value update
    $$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_k+1}v_k$$
    - $v_k$是state value吗？不是，$v_k$是一个值，但不确定其是否满足Bellman equation

- 理解原理常使用elementwise form（编程实现常使用matrix form）
    - Step1: Policy update
    - The elementwise form of
    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k)$$
    is
    $$\pi_{k+1}(s)=\arg \max_\pi\sum_a\pi(a|s)\underbrace{\left(\sum_r p(r|s,a)r+\gamma\sum_{s'} p(s'|s,a)v(s')\right)}_{q(s,a)},\quad s\in \mathcal{S}$$
    - 解决上述问题的最优策略是
    $$\pi_{k+1}(a|s)=\begin{cases}
        1\quad a=a_k^\ast(s) \\ 0 \quad a\neq a_k^\ast(s)
    \end{cases}$$
    其中$a_k^\ast(s)=\arg\max_aq_k(a,s)$。$\pi_{k+1}$被称为greedy policy，因为它总是选择最大的q值
    - Step2: Value update
    - The elementwise form of
    $$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k$$
    is
    $$v_{k+1}(s)=\sum_a\pi_{k+1}(a|s)\underbrace{\left(\sum_r p(r|S,a)r+\gamma \sum_{s'}p(s'|s,a)v_k(s')\right)}_{q_k(s,a)}, \qquad s\in\mathcal S$$
    由于$\pi_{k+1}$是greedy的，上述方程可简化为
    $$v_{k+1}(s)=\max_a q_k(a,s)$$
    - 过程总结
    $$v_k(s)\to q_k(s,a)\to \text{greedy policy } \pi_{k+1}\to \text{new value }v_{k+1}=\max_a q_k(s,a)$$
    - 伪代码
        - 初始化：概率模型$p(r|s,a)$和$p(s'|s,a)$对于所有$(s,a)$均已知，初始假设$v_0$
        - 目标：解BOE寻找最优state value和最优policy
        - 当$v_k$尚未收敛，即$||v_k-v_{k-1}||$小于给定的最小阈值，则对于第$k^{th}$次迭代，do
          - For every state $s \in \mathcal S$, do
            - For every action $a\in \mathcal A(s)$, do
              - q-value:$q_k(s,a)=\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_k(s')$
            - Maximum action value: $a_k^\ast(s)=\arg\max_a q_k(a,s)$
            - Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^\ast$ and $\pi_{k+1}(a|s)=0$ otherwise
            - Value update: $v_{k+1}(s)=\max_a q_k(a,s)$

## 4.2 Policy iteration algorithm

- Algorithm description:
    - 初始任意给定策略$\pi_0$，再迭代求解最优策略
    - Step1: policy evaluation(PE)
    该步骤是计算$\pi_k$的state value
    $$v_{\pi_k}+r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}$$
    注意$v_{\pi_k}$是一个state value function
    - Step2: policy improvement(PI)
    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$$
    - 迭代过程
    $$\pi_0 \xrightarrow{PE} v_{\pi_0} \xrightarrow{PI} \pi_1 \xrightarrow{PE} v_{\pi_1} \xrightarrow{PI} \pi_2 \xrightarrow{PE} v_{\pi_2} \xrightarrow{PI}...$$
    - PE=policy evaluation, PI=policy improvement

- Q1: PE步骤中如何求解Bellman equation得到state value $v_{\pi_k}$？
    $$v_{\pi_k}+r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}$$
    - Closed-form solution:
    $$v_{\pi_k}=(I-\gamma P_{\pi_k})^{-1}r_{\pi_k}$$
    - Iterative solution:
    $$v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)},\quad j=0,1,2,...$$
    - Iterative solution中也是一个小迭代，因此有两重循环

- Q2: PI步骤中，为什么新策略$\pi_{k+1}$一定优于$\pi_k$？
    - 引理
    如果$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$，那么有$v_{\pi_{k+1}}\geq v_{\pi_k}$对于任意$k$成立

- Q3: 为什么这种迭代算法最终可以到达最优策略？
    - 由于每次迭代都能优化策略，有
    $$v_{\pi_0} \leq v_{\pi_1} \leq v_{\pi_2} \leq ... \leq v_{\pi_k} \leq ... \leq v^\ast$$
    最终$v_{\pi_k}$会增长且收敛，但仍需证明收敛于$v^\ast$
    - Theorem(Convergence of Policy Iteration)
    由policy iteration生成的state value序列$\{ v_{\pi_k}\}_{k=0}^\infty$算法最终会收敛于最优state value $v^\ast$. 最终，policy sequence$\{ \pi_k\}_{k=0}^\infty$最终收敛于最优策略

- Q4: policy iteration 和 value iteration的关系？
    - 与Q3有关。是Trucated Iteration的极端情况。

- 算法实现(Elementwise form)
    - Step 1: Policy evaluation
    - Matrix-vector form: $v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k}v_{\pi_k}^{(j)}, \quad j=0,1,2,...$
    - Elementwise form:
    $$v_{\pi_k}^{(j+1)}(s)=\sum_a\pi_k(a|s) \left(\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)\textcolor{blue}{v_{\pi_k}^{(j)}(s')}\right), \qquad s\in\mathcal S$$
    当$j\to\infty$或$j$足够大或$||v_{\pi_k}^{(j+1)}-v_{\pi_k}^{(j)}||$足够小时算法结束（收敛）
    - Step 2: Policy improvement
    - Matrix-vector form: $\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_{\pi_k})$
    - Elementwise form
    $$\pi_{k+1}(s)=\arg\max_\pi\sum_a\pi_k(a|s)\underbrace{\left(\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_k(s')\right)}_{q_{\pi_k}(s,a)}, \qquad s\in\mathcal S$$
    式中，$q_{\pi_k}(s,a)$是$\pi_k$策略下的action value，令
    $$a_k^\ast(s)=\arg\max_a q_{\pi_k}(s,a)$$
    那么，greedy policy是
    $$\pi_{k+1}(a|s)=\begin{cases}
      1\quad a=a_k^\ast(s) \\ 0 \quad a\neq a_k^\ast(s)
    \end{cases}$$
    - 伪代码
        - 初始化：概率模型$p(r|s,a)$和$p(s'|s,a)$对于所有$(s,a)$均已知，初始假设$\pi_0$
        - 目标：寻找最优state value和最优policy
        - 当策略尚未收敛，则对于第$k^{th}$次迭代，do
          - Policy evaluation:
          - Initialization: 任意初始猜测值$v_{\pi_k}^{(0)}$
          - While $v_{\pi_k}^{(j)}$ has not converged, for $j^{th}$ iteration, do
            - For every state $s\in \mathcal{S}$ ,do 
              - $v_{\pi_k}^{(j+1)}(s)=\sum_a\pi_k(a|s) \left[\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_{\pi_k}^{(j)}(s')\right]$
          - Policy improvement:
          - For every state $s\in \mathcal{S}$, do
            - For every action $a\in \mathcal{A}(s)$, do
              - $q_{\pi_k}(s,a)=\sum_r p(r|s,a)r+\gamma \sum_{s'} p(s'|s,a)v_{\pi_k}(s')$
              - $a_k^\ast(s)=\arg\max_a q_{\pi_k}(s,a)$
              - $\pi_{k+1}(a|s)=1$ if $a=a_k^\ast$, and $\pi_{k+1}(a|s)=0$ otherwise

- 实例分析
    - 接近目标的策略先变好，因为选择策略时严重依赖于其他策略
    - 如果其他策略混乱则不能到达目标；反之当其他策略可以到达目标，那么当前迭代的策略也能到达目标