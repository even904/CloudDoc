---
author: [Even]
date: [2026年02月28日]
update: [2026年02月28日]
title: [【强化学习】4 值迭代与策略迭代]
tags: [强化学习,数学原理,值迭代,策略迭代,截断策略迭代]
---

# 4 值迭代与策略迭代

## 4.1 Value iteration algorithm

- 解决BOE问题
    - BOE表达式
    $$v=f(v)=\max_\pi(r_\pi+\gamma P_\pi v)$$
    - Contraction mapping theorem
    $$v_{k+1}=f(v_k)=\max_\pi(r_\pi+\gamma P_\pi v_k), \quad k=1,2,3...$$
    其中$v_0$可为任意值
    - 以上算法称为值迭代，最终可以找到最优值和最优策略
    - 以上算法可以分为两步完成
    - Step1: policy update
    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k)$$
    其中$v_k$给定
    - Step2: value update
    $$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_k+1}v_k$$
    - $v_k$是state value吗？不是，$v_k$是一个值，但不确定其是否满足Bellman equation

- 理解原理常使用elementwise form（编程实现常使用matrix form）
    - Step1: Policy update
    - The elementwise form of
    $$\pi_{k+1}=\arg\max_\pi(r_\pi+\gamma P_\pi v_k)$$
    is
    $$\pi_{k+1}(s)=\arg \max_\pi\sum_a\pi(a|s)\underbrace{\left(\sum_r p(r|s,a)r+\gamma\sum_{s'} p(s'|s,a)v(s')\right)}_{q(s,a)},\quad s\in \mathcal{S}$$
    - 解决上述问题的最优策略是
    $$\pi_{k+1}(a|s)=\begin{cases}
        1\quad a=a_k^\ast(s) \\ 0 \quad a\neq a_k^\ast(s)
    \end{cases}$$
    其中$a_k^\ast(s)=\arg\max_aq_k(a,s)$。$\pi_{k+1}$被称为greedy policy，因为它总是选择最大的q值
    - Step2: Value update
    - The elementwise form of
    $$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_k$$
    is
    $$v_{k+1}(s)=\sum_a\pi_{k+1}(a|s)\underbrace{\left(\sum_r p(r|S,a)r+\gamma \sum_{s'}p(s'|s,a)v_k(s')\right)}_{q_k(s,a)}, \qquad s\in\mathcal S$$
    由于$\pi_{k+1}$是greedy的，上述方程可简化为
    $$v_{k+1}(s)=\max_a q_k(a,s)$$
    - 过程总结
    $$v_k(s)\to q_k(s,a)\to \text{greedy policy } \pi_{k+1}\to \text{new value }v_{k+1}=\max_a q_k(s,a)$$
    - 伪代码
        - 初始化：概率模型$p(r|s,a)$和$p(s'|s,a)$对于所有$(s,a)$均已知，初始假设$v_0$
        - 目标：解BOE寻找最优state value和最优policy
        - 当$v_k$尚未收敛，即$||v_k-v_{k-1}||$小于给定的最小阈值，则对于第$k^{th}$次迭代，do
          - For every state $s \in \mathcal S$, do
            - For every action $a\in \mathcal A(s)$, do
              - q-value:$q_k(s,a)=\sum_r p(r|s,a)r+\gamma \sum_{s'}p(s'|s,a)v_k(s')$
            - Maximum action value: $a_k^\ast(s)=\arg\max_a q_k(a,s)$
            - Policy update: $\pi_{k+1}(a|s)=1$ if $a=a_k^\ast$ and $\pi_{k+1}(a|s)=0$ otherwise
            - Value update: $v_{k+1}(s)=\max_a q_k(a,s)$